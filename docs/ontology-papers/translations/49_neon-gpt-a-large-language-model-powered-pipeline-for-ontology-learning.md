# [49] NeOn-GPT: LLM 기반 온톨로지 학습 파이프라인

- 영문 제목: NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning
- 연도: 2025
- 원문 링크: https://doi.org/10.1007/978-3-031-78952-6_4
- DOI: 10.1007/978-3-031-78952-6_4
- 원문 저장 상태: html_saved
- 원문 파일: /Volumes/SAMSUNG/apps/projects/vive-md/docs/ontology-papers/originals/49_neon-gpt-a-large-language-model-powered-pipeline-for-ontology-learning.html
- 번역 상태: partial_translated

## 원문(추출 텍스트)

NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning | Springer Nature Link
Skip to main content
Advertisement
Log in
Menu
Find a journal
Publish with us
Track your research
Search
Saved research
Cart
Home
The Semantic Web: ESWC 2024 Satellite Events
Conference paper
NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning
Conference paper
First Online:
28 January 2025
pp 36–50
Cite this conference paper
The Semantic Web: ESWC 2024 Satellite Events
(ESWC 2024)
Nadeen Fathallah
21
,
Arunav Das
22
,
Stefano De Giorgis
23
,
Andrea Poltronieri
24
,
Peter Haase
25
&
…
Liubov Kovriguina
25
Show authors
Part of the book series:
Lecture Notes in Computer Science
((LNCS,volume 15344))
Included in the following conference series:
European Semantic Web Conference
1002
Accesses
15
Citations
Abstract
We address the task of ontology learning by combining the structured NeOn methodology framework with Large Language Models (LLMs) for translating natural language domain descriptions into Turtle syntax ontologies. The main contribution of the paper is a prompt pipeline tailored for domain-agnostic modeling, exemplified through the application to a domain-specific case study: the wine ontology. The resulting pipeline is used to develop NeOn-GPT, a workflow for automatic ontology modeling, and its proof of concept implementation, integrated on top of the metaphactory platform. NeOn-GPT leverages the systematic approach of the NeOn methodology and LLMs’ generative capabilities to facilitate a more efficient ontology development process. We evaluate the proposed approach by conducting comprehensive evaluations using the Stanford wine ontology as the gold standard. The obtained results show, that LLMs are not fully equipped to perform procedural tasks required for ontology development, and lack the reasoning skills and domain expertise needed. Overall, LLMs require integration with the workflow or trajectory tools for continuous knowledge engineering tasks. Nevertheless, LLMs can significantly alleviate the time and expertise needed. Our code base is publicly available for research and development purposes, accessible at:
https://github.com/andreamust/NEON-GPT
.
Conceptualization:
N.F., A.D, S.D.G, A.P, P.H;
Investigation:
N.F., A.D, S.D.G, A.P, P.H;
Methodology:
S.D.G, A.P, P.H;
Software:
N.F, A.D, S.D.G, A.P;
Validation:
N.F, A.D;
Visualization:
N.F, A.D;
Writing - original draft:
N.F., A.D;
Writing - review & editing:
N.F, A.D, S.D.G., L.K.
This is a preview of subscription content,
log in via an institution
to check access.
Access this chapter
Log in via an institution
Subscribe and save
Springer+
from €37.37 /Month
Starting from 10 chapters or articles per month
Access and download chapters and articles from more than 300k books and 2,500 journals
Cancel anytime
View plans
Buy Now
Chapter
EUR 29.95
Price includes VAT (Korea(Rep.))
Available as PDF
Read on any device
Instant download
Own it forever
Buy Chapter
eBook
EUR 53.49
Price includes VAT (Korea(Rep.))
Available as EPUB and PDF
Read on any device
Instant download
Own it forever
Buy eBook
Softcover Book
EUR 65.99
Price excludes VAT (Korea(Rep.))
Compact, lightweight edition
Dispatched in 3 to 5 business days
Free shipping worldwide -
see info
Buy Softcover Book
Tax calculation will be finalised at checkout
Purchases are for personal use only
Institutional subscriptions
Similar content being viewed by others
Navigating Ontology Development with Large Language Models
Chapter
© 2024
Unleashing the power of untuned large language models in recommender systems: a thorough investigation of current approaches, challenges, and future research directions
Article
16 July 2025
Large Language Models as Booster in Pattern-Based Ontology Transformation
Chapter
© 2026
Explore related subjects
Discover the latest articles, books and news in related subjects, suggested using machine learning.
Gene ontology
Ontology
Machine Translation
Machine Learning
Open Source
Technical Languages
Knowledge Graphs and Semantic Data Integration
References
Auer, S.: The RapidOWL methodology–towards agile knowledge engineering. In: 15th IEEE International Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises (WETICE 2006), 26-28 June 2006, Manchester, United Kingdom, pp. 352–357. IEEE Computer Society (2006).
https://doi.org/10.1109/WETICE.2006.67
,
https://doi.org/10.1109/WETICE.2006.67
Aussenac-Gilles, N., Biebow, B., Szulman, S.: Revisiting ontology design: a methodology based on corpus analysis. In: Dieng, R., Corby, O. (eds.) Knowledge Acquisition, Modeling and Management, 12th International Conference, EKAW 2000, Juan-les-Pins, France, October 2-6, 2000, Proceedings. Lecture Notes in Computer Science, vol. 1937, pp. 172–188. Springer (2000).
https://doi.org/10.1007/3-540-39967-4_13
Beckett, D., Berners-Lee, T., Prud’hommeaux, E., Carothers, G.: RDF 1.1 Turtle. World Wide Web Consortium, pp. 18–31 (2014)
Google Scholar
Bikeyev, A.: Synthetic ontologies: a hypothesis. Available at SSRN 4373537 (2023)
Google Scholar
Chowdhery, A., et al.: PaLM: scaling language modeling with pathways. J. Mach. Learn. Res. (JMLR)
24
, 240:1–240:113 (2023).
http://jmlr.org/papers/v24/22-1144.html
Cimiano, P., Völker, J.: Text2Onto. In: Montoyo, A., Muńoz, R., Métais, E. (eds.) NLDB 2005. LNCS, vol. 3513, pp. 227–238. Springer, Heidelberg (2005).
https://doi.org/10.1007/11428817_21
Chapter
MATH
Google Scholar
Fill, H., Fettke, P., Köpke, J.: Conceptual modeling and large language models: impressions from first experiments with ChatGPT. Enterprise Modelling and Information Systems Architectures International Journal of Conceptual Modelling (EMISAJ)
18
, 3 (2023).
https://doi.org/10.18417/EMISA.18.3
,
https://doi.org/10.18417/emisa.18.3
Gao, P., et al.: LLaMA-adapter V2: parameter-efficient visual instruction model. arXiv preprint
arXiv:2304.15010
(2023)
Glimm, B., Horrocks, I., Motik, B., Stoilos, G., Wang, Z.: HermiT: an OWL 2 reasoner. J. Autom. Reason.
53
, 245–269 (2014)
Article
MATH
Google Scholar
Haase, P., Herzig, D.M., Kozlov, A., Nikolov, A., Trame, J.: metaphactory: a platform for knowledge graph management. Seman. Web
10
(6), 1109–1125 (2019).
https://doi.org/10.3233/SW-190360
Haase, P., Lewen, H., Studer, R., Tran, D.T., Erdmann, M., d’Aquin, M., Motta, E.: The neon ontology engineering toolkit. World Wide Web J. (WWW) (2008)
Google Scholar
Hearst, M.: Automated discovery of WordNet relations. WordNet an electronic lexical database (1998)
Google Scholar
Ji, Z., Yu, T., Xu, Y., Lee, N., Ishii, E., Fung, P.: Towards mitigating LLM hallucination via self reflection. In: Bouamor, H., Pino, J., Bali, K. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 1827–1843. Association for Computational Linguistics (2023).
https://aclanthology.org/2023.findings-emnlp.123
Joshi, I., Budhiraja, R., Akolekar, H.D., Challa, J.S., Kumar, D.: It’s not like Jarvis, but it’s pretty close! - examining ChatGPT’s usage among undergraduate students in computer science. CoRR abs/2311.09651 (2023).
https://doi.org/10.48550/ARXIV.2311.09651
Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models are zero-shot reasoners. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 (2022).
http://papers.nips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html
Krech, D., et al.: RDFLib (2023).
https://doi.org/10.5281/zenodo.6845245
,
https://github.com/RDFLib/rdflib
Maedche, A., Staab, S.: Ontology learning for the semantic web. IEEE Intell. Syst.
16
(2), 72–79 (2001).
https://doi.org/10.1109/5254.920602
Marcus, G.: Sora’s surreal physics.
https://garymarcus.substack.com/p/soras-surreal-physics
(2024). Accessed 27 Feb 2024
McGuinness, D.L., Van Harmelen, F., et al.: OWL web ontology language overview. World Wide Web Consortium recommendation
10
(10), 2004 (2004)
Google Scholar
Musen, M.A.: The protégé project: a look back and a look forward. AI Matters
1
(4), 4–12 (2015)
Google Scholar
Neuhaus, F.: Ontologies in the era of large language models - a perspective. Appl. Ontol.
18
(4), 399–407 (2023).
https://doi.org/10.3233/AO-230072
Noy, N.F., McGuinness, D.L., et al.: Ontology development 101: a guide to creating your first ontology (2001)
Google Scholar
Peroni, S.: A simplified agile methodology for ontology development. In: Dragoni, M., Poveda-Villalón, M., Jiménez-Ruiz, E. (eds.) OWL: - Experiences and Directions - Reasoner Evaluation - 13th International Workshop, OWLED 2016, and 5th International Workshop, ORE 2016, Bologna, Italy, November 20, 2016, Revised Selected Papers. Lecture Notes in Computer Science, vol. 10161, pp. 55–69. Springer (2016).
https://doi.org/10.1007/978-3-319-54627-8_5
Petroni, F., et al.: Language models as knowledge bases? In: Inui, K., Jiang, J., Ng, V., Wan, X. (eds.) Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3–7, 2019, pp. 2463–2473. Association for Computational Linguistics (2019).
https://doi.org/10.18653/V1/D19-1250
Poveda-Villalón, M., Gómez-Pérez, A., Suárez-Figueroa, M.C.: Oops! (ontology pitfall scanner!): an on-line tool for ontology evaluation. Int. J. Seman. Web Inf. Syst. (IJSWIS)
10
(2), 7–34 (2014).
https://doi.org/10.4018/IJSWIS.2014040102
Presutti, V., Daga, E., Gangemi, A., Blomqvist, E.: extreme design with content ontology design patterns. In: Blomqvist, E., Sandkuhl, K., Scharffe, F., Svátek, V. (eds.) Proceedings of the Workshop on Ontology Patterns (WOP 2009), collocated with the 8th International Semantic Web Conference ( ISWC-2009 ), Washington D.C., USA, 25 October, 2009. CEUR Workshop Proceedings, vol. 516. CEUR-WS.org (2009).
https://ceur-ws.org/Vol-516/pap21.pdf
Quelle, D., Bovet, A.: The perils & promises of fact-checking with large language models. CoRR abs/2310.13549 (2023).
https://doi.org/10.48550/arXiv.2310.13549
Shanahan, M., McDonell, K., Reynolds, L.: Role play with large language models. Nature
623
(7987), 493–498 (2023).
https://doi.org/10.1038/S41586-023-06647-8
Smith, M., Zorpette, G., Choi, C.Q., Boyd, J.: Generative AI slims down for a portable world:
\(>\)
consumer tech aims LLMs everywhere-with laptops as the beachhead. IEEE Spectrum
61
(2), 5–13 (2024)
Google Scholar
Spoladore, D., Pessot, E., Trombetta, A.: A novel agile ontology engineering methodology for supporting organizations in collaborative ontology development. Comput. Industry
151
, 103979 (2023).
https://doi.org/10.1016/J.COMPIND.2023.103979
Suárez-Figueroa, M.C., Gómez-Pérez, A., Fernández-López, M.: The neon methodology framework: a scenario-based methodology for ontology development. Appl. Ontol.
10
(2), 107–145 (2015).
https://doi.org/10.3233/AO-150145
Tang, J., et al.: GraphGPT: graph instruction tuning for large language models. CoRR abs/2310.13023 (2023).
https://doi.org/10.48550/ARXIV.2310.13023
Tello, A.L., Gómez-Pérez, A.: ONTOMETRIC: a method to choose the appropriate ontology. J. Database Manage. (JDM)
15
(2), 1–18 (2004).
https://doi.org/10.4018/JDM.2004040101
Wei, J., et al.: Emergent abilities of large language models. Trans. Mach. Learn. Res. (TMLR)
2022
(2022).
https://openreview.net/forum?id=yzkSU5zdwD
Wei, J., et al.: Chain-of-thought prompting elicits reasoning in large language models. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 (2022).
http://papers.nips.cc/paper_files/paper/2022/hash/

## 한국어 번역

NeOn-GPT: 온톨로지 학습을 위한 대규모 언어 모델 기반 파이프라인 | 스프링거 네이처 링크
주요 콘텐츠로 건너뛰기
광고
로그인
메뉴
저널 찾기
우리와 함께 출판하세요
연구 추적
검색
저장된 연구
장바구니
홈
시맨틱 웹: ESWC 2024 위성 이벤트
컨퍼런스 페이퍼
NeOn-GPT: 온톨로지 학습을 위한 대규모 언어 모델 기반 파이프라인
컨퍼런스 페이퍼
첫 번째 온라인:
2025년 1월 28일
36~50쪽
이 컨퍼런스 논문을 인용하세요
시맨틱 웹: ESWC 2024 위성 이벤트
(ESWC 2024)
나딘 파탈라
21
,
아루나브 다스
22
,
스테파노 데 조르지스
23
,
안드레아 폴트로니에리
24
,
피터 하세
25
&
…
리우보프 코브리기나
25
작성자 표시
책 시리즈의 일부:
컴퓨터 과학 강의 노트
((LNCS, 15344권))
다음 컨퍼런스 시리즈에 포함되어 있습니다:
유럽 시맨틱 웹 컨퍼런스
1002
액세스
15
인용
초록
우리는 자연어 도메인 설명을 Turtle 구문 온톨로지로 변환하기 위해 구조화된 NeOn 방법론 프레임워크와 LLM(Large Language Models)을 결합하여 온톨로지 학습 작업을 다룹니다. 이 논문의 주요 기여는 도메인별 사례 연구인 와인 온톨로지에 대한 적용을 통해 예시된 도메인 독립적 모델링에 맞춰진 신속한 파이프라인입니다. 결과 파이프라인은 자동 온톨로지 모델링을 위한 워크플로우인 NeOn-GPT와 메타팩토리 플랫폼 위에 통합된 개념 증명 구현을 개발하는 데 사용됩니다. NeOn-GPT는 NeOn 방법론의 체계적인 접근 방식과 LLM의 생성 기능을 활용하여 보다 효율적인 온톨로지 개발 프로세스를 촉진합니다. 우리는 스탠포드 와인 온톨로지를 표준으로 사용하여 포괄적인 평가를 수행하여 제안된 접근 방식을 평가합니다. 얻은 결과는 LLM이 온톨로지 개발에 필요한 절차적 작업을 수행할 수 있는 충분한 장비를 갖추고 있지 않으며 필요한 추론 기술과 도메인 전문 지식이 부족하다는 것을 보여줍니다. 전반적으로 LLM은 지속적인 지식 엔지니어링 작업을 위해 워크플로 또는 궤적 도구와의 통합이 필요합니다. 그럼에도 불구하고 LLM은 필요한 시간과 전문 지식을 크게 줄일 수 있습니다. 우리의 코드 베이스는 연구 및 개발 목적으로 공개적으로 제공되며 다음 주소에서 액세스할 수 있습니다.
https://github.com/andreamust/NEON-GPT
.
개념화:
N.F., A.D, S.D.G, A.P, P.H;
조사:
N.F., A.D, S.D.G, A.P, P.H;
방법론:
SDG, A.P, P.H;
소프트웨어:
N.F, A.D, S.D.G, A.P;
검증:
NF, AD;
시각화:
NF, AD;
집필 - 원본 초안:
NF, AD;
집필 - 검토 및 편집:
N.F, A.D, S.D.G., L.K.
구독 콘텐츠 미리보기 입니다,
기관을 통해 로그인
액세스를 확인합니다.
이 장에 액세스
기관을 통해 로그인
구독하고 저장하세요
스프링거+
₩37.37/월부터
매월 10개의 장 또는 기사부터 시작
30만 권 이상의 도서와 2,500개 이상의 저널에서 장과 기사에 액세스하고 다운로드하세요.
언제든지 취소
계획 보기
지금 구매
장
EUR 29.95
가격에는 VAT가 포함되어 있습니다(대한민국)
PDF로 사용 가능
모든 기기에서 읽기
즉시 다운로드
영원히 소유하세요
챕터 구매
전자책
EUR 53.49
가격에는 VAT가 포함되어 있습니다(대한민국)
EPUB 및 PDF로 사용 가능
모든 기기에서 읽기
즉시 다운로드
영원히 소유하세요
eBook 구매
소프트커버 책
EUR 65.99
VAT 별도 가격 (대한민국)
컴팩트하고 가벼운 버전
영업일 기준 3~5일 이내에 발송됩니다.
전 세계 무료 배송 -
정보 보기
소프트커버 도서 구매
세금 계산은 결제 시 완료됩니다.
구매는 개인 용도로만 가능합니다.
기관 구독
다른 사람들이 유사한 콘텐츠를 보고 있음
대규모 언어 모델을 사용한 온톨로지 개발 탐색
장
© 2024
추천 시스템에서 조정되지 않은 대규모 언어 모델의 힘 활용: 현재 접근 방식, 과제 및 향후 연구 방향에 대한 철저한 조사
기사
2025년 7월 16일
패턴 기반 온톨로지 변환의 부스터 역할을 하는 대규모 언어 모델
장
© 2026
관련 주제 탐색
머신러닝을 활용하여 추천되는 관련 주제의 최신 기사, 도서, 뉴스를 찾아보세요.
유전자 온톨로지
온톨로지
기계 번역
기계 학습
오픈 소스
기술 언어
지식 그래프와 의미론적 데이터 통합
참고자료
Auer, S.: 민첩한 지식 엔지니어링을 향한 RapidOWL 방법론. In: 기술 활성화에 관한 제15회 IEEE 국제 워크숍: 협업 기업을 위한 인프라(WETICE 2006), 2006년 6월 26~28일, 영국 맨체스터, pp. 352–357. IEEE 컴퓨터 학회(2006).
https://doi.org/10.1109/WETICE.2006.67
,
https://doi.org/10.1109/WETICE.2006.67
Aussenac-Gilles, N., Biebow, B., Szulman, S.: 온톨로지 설계 재검토: 말뭉치 분석에 기반한 방법론. In: Dieng, R., Corby, O. (eds.) 지식 획득, 모델링 및 관리, 제12차 국제 회의, EKAW 2000, Juan-les-Pins, 프랑스, ​​2000년 10월 2-6일, 절차. 컴퓨터 과학 강의 노트, vol. 1937년, 172~188페이지. 스프링거(2000).
https://doi.org/10.1007/3-540-39967-4_13
Beckett, D., Berners-Lee, T., Prud'hommeaux, E., Carothers, G.: RDF 1.1 Turtle. 월드 와이드 웹 컨소시엄, pp. 18–31(2014)
구글 학술검색
Bikeyev, A.: 합성 온톨로지: 가설. SSRN 4373537(2023)에서 사용 가능
구글 학술검색
Chowdhery, A., 외: PaLM: 경로를 통한 언어 모델링 확장. J. 마하. 배우다. 결의안. (JMLR)
24
, 240:1~240:113(2023).
http://jmlr.org/papers/v24/22-1144.html
Cimiano, P., Völker, J.: Text2Onto. In: Montoyo, A., Muńoz, R., Métais, E. (eds.) NLDB 2005. LNCS, vol. 3513, pp. 227–238. 스프링거, 하이델베르그(2005).
https://doi.org/10.1007/11428817_21
장
수학
구글 학술검색
Fill, H., Fettke, P., Köpke, J.: 개념 모델링 및 대규모 언어 모델: ChatGPT를 사용한 첫 번째 실험에서 받은 인상. 엔터프라이즈 모델링 및 정보 시스템 아키텍처 국제 개념 모델링 저널(EMISAJ)
18
, 3(2023).
https://doi.org/10.18417/EMISA.18.3
,
https://doi.org/10.18417/emisa.18.3
Gao, P. 등: LLaMA 어댑터 V2: 매개변수 효율적인 시각적 교육 모델. arXiv 사전 인쇄
arXiv:2304.15010
(2023)
Glimm, B., Horrocks, I., Motik, B., Stoilos, G., Wang, Z.: HermiT: OWL 2 추론기. J.Autom. 이유.
53
, 245–269 (2014)
기사
수학
구글 학술검색
Haase, P., Herzig, D.M., Kozlov, A., Nikolov, A., Trame, J.: 메타팩토리: 지식 그래프 관리를 위한 플랫폼. Seman. 웹
10
(6), 1109~1125(2019).
https://doi.org/10.3233/SW-190360
Haase, P., Lewen, H., Studer, R., Tran, D.T., Erdmann, M., d'Aquin, M., Motta, E.: 네온 온톨로지 엔지니어링 툴킷. 월드와이드웹 J. (WWW) (2008)
구글 학술검색
Hearst, M.: WordNet 관계 자동 검색. 전자 어휘 데이터베이스 WordNet(1998)
구글 학술검색
Ji, Z., Yu, T., Xu, Y., Lee, N., Ishii, E., Fung, P.: 자기 성찰을 통해 LLM 환각을 완화하는 방향. In: Bouamor, H., Pino, J., Bali, K. (eds.) 전산 언어학 협회 조사 결과: EMNLP 2023, 싱가포르, 2023년 12월 6-10일, pp. 1827-1843. 전산언어학협회(2023).
https://aclanthology.org/2023.findings-emnlp.123
Joshi, I., Budhiraja, R., Akolekar, H.D., Challa, J.S., Kumar, D.: Jarvis와는 다르지만 매우 가깝습니다! - 컴퓨터 과학 학부생의 ChatGPT 사용을 조사합니다. CoRR abs/2311.09651 (2023).
https://doi.org/10.48550/ARXIV.2311.09651
Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: 대규모 언어 모델은 제로샷 추론기입니다. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) 신경 정보 처리 시스템의 발전 35: 신경 정보 처리 시스템에 관한 연례 컨퍼런스 2022, NeurIPS 2022, 뉴올리언스, LA, 미국, 2022년 11월 28일 - 12월 9일(2022).
http://papers.nips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html
Krech, D. 등: RDFLib(2023).
https://doi.org/10.5281/zenodo.6845245
,
https://github.com/RDFLib/rdflib
Maedche, A., Staab, S.: 시맨틱 웹을 위한 온톨로지 학습. IEEE 인텔. 시스템.
16
(2), 72-79(2001).
https://doi.org/10.1109/5254.920602
Marcus, G.: 소라의 초현실적인 물리학.
https://garymarcus.substack.com/p/soras-surreal-physics
(2024). 2024년 2월 27일에 액세스함
McGuinness, D.L., Van Harmelen, F. 등: OWL 웹 온톨로지 언어 개요. 월드와이드웹 컨소시엄 추천
10
(10), 2004 (2004)
구글 학술검색
Musen, M.A.: 제자 프로젝트: 되돌아보기와 기대하기. AI 문제
1
(4), 4~12(2015)
구글 학술검색
Neuhaus, F.: 대규모 언어 모델 시대의 온톨로지 - 관점. 신청 온톨.
18
(4), 399-407(2023).
https://doi.org/10.3233/AO-230072
Noy, N.F., McGuinness, D.L., 외: 온톨로지 개발 101: 첫 번째 온톨로지 생성 가이드(2001)
구글 학술검색
Peroni, S.: 온톨로지 개발을 위한 단순화된 애자일 방법론. In: Dragoni, M., Poveda-Villalón, M., Jiménez-Ruiz, E. (eds.) OWL: - 경험 및 방향 - 추론기 평가 - 제13차 국제 워크숍, OWLED 2016 및 제5차 국제 워크숍, ORE 2016, 볼로냐, 이탈리아, 2016년 11월 20일, 선별된 논문 수정. 컴퓨터 과학 강의 노트, vol. 10161, 55~69페이지. 스프링거(2016).
https://doi.org/10.1007/978-3-319-54627-8_5
Petroni, F., et al.: 지식 기반으로서의 언어 모델? In: Inui, K., Jiang, J., Ng, V., Wan, X. (eds.) 2019년 자연어 처리의 경험적 방법 컨퍼런스 및 제9차 자연어 처리에 관한 국제 합동 컨퍼런스 간행물, EMNLP-IJCNLP 2019, 중국 홍콩, 2019년 11월 3~7일, pp. 2463-2473. 전산언어학협회(2019).
https://doi.org/10.18653/V1/D19-1250
Poveda-Villalon, M., Gómez-Pérez, A., Suárez-Figueroa, M.C.: 이런! (온톨로지 함정 스캐너!): 온톨로지 평가를 위한 온라인 도구입니다. 국제 J. Seman. 웹 정보 시스템. (IJSWIS)
10
(2), 7~34(2014).
https://doi.org/10.4018/IJSWIS.2014040102
Presutti, V., Daga, E., Gangemi, A., Blomqvist, E.: 콘텐츠 온톨로지 디자인 패턴을 사용한 극단적인 디자인. In: Blomqvist, E., Sandkuhl, K., Scharffe, F., Svátek, V. (eds.) 온톨로지 패턴에 관한 워크숍 진행(WOP 2009), 2009년 10월 25일 미국 워싱턴 D.C.에서 열린 제8차 국제 의미 웹 컨퍼런스(ISWC-2009)와 함께 개최됨. CEUR 워크숍 절차, vol. 516. CEUR-WS.org(2009).
https://ceur-ws.org/Vol-516/pap21.pdf
Quelle, D., Bovet, A.: 대규모 언어 모델을 사용한 사실 확인의 위험과 약속. CoRR abs/2310.13549 (2023).
https://doi.org/10.48550/arXiv.2310.13549
Shanahan, M., McDonell, K., Reynolds, L.: 대규모 언어 모델을 사용한 역할극. 자연
623
(7987), 493–498 (2023).
https://doi.org/10.1038/S41586-023-06647-8
Smith, M., Zorpette, G., Choi, C.Q., Boyd, J.: Generative AI는 휴대용 세계를 위해 슬림화됩니다.
\(>\)
소비자 기술은 노트북을 교두보로 삼아 어디에서나 LLM을 목표로 합니다. IEEE 스펙트럼
61
(2), 5~13(2024)
구글 학술검색
Spoladore, D., Pessot, E., Trombetta, A.: 협업 온톨로지 개발에서 조직을 지원하기 위한 새롭고 민첩한 온톨로지 엔지니어링 방법론입니다. 계산. 산업
151
, 103979(2023).
https://doi.org/10.1016/J.COMPIND.2023.103979
Suárez-Figueroa, M.C., Gómez-Pérez, A., Fernández-López, M.: 네온 방법론 프레임워크: 온톨로지 개발을 위한 시나리오 기반 방법론. 신청 온톨.
10
(2), 107–145(2015).
https://doi.org/10.3233/AO-150145
Tang, J., et al.: GraphGPT: 대규모 언어 모델을 위한 그래프 명령 조정. CoRR abs/2310.13023 (2023).
https://doi.org/10.48550/ARXIV.2310.13023
Tello, A.L., Gómez-Pérez, A.: ONTOMETRIC: 적절한 온톨로지를 선택하는 방법입니다. J. 데이터베이스 관리. (JDM)
15
(2), 1–18(2004).
https://doi.org/10.4018/JDM.2004040101
Wei, J., et al.: 대규모 언어 모델의 새로운 능력. 트랜스. 마하. 배우다. 결의안. (TMLR)
2022년
(2022).
https://openreview.net/forum?id=yzkSU5zdwD
Wei, J., et al.: 생각의 연쇄 유도는 대규모 언어 모델에서 추론을 이끌어냅니다. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) 신경 정보 처리 시스템의 발전 35: 신경 정보 처리 시스템에 관한 연례 컨퍼런스 2022, NeurIPS 2022, 뉴올리언스, LA, 미국, 2022년 11월 28일 - 12월 9일(2022).
http://papers.nips.cc/paper_files/paper/2022/hash/
