# [58] ì˜¨í†¨ë¡œì§€ í•™ìŠµì˜ LLM ì „í™˜ ë™í–¥ ë‹¨ê¸° ë¦¬ë·°

- ì˜ë¬¸ ì œëª©: A Short Review for Ontology Learning: Stride to Large Language Models Trend
- ì—°ë„: 2024
- ì›ë¬¸ ë§í¬: https://doi.org/10.48550/arXiv.2404.14991
- DOI: 10.48550/arxiv.2404.14991
- ì›ë¬¸ ì €ì¥ ìƒíƒœ: pdf_saved
- ì›ë¬¸ íŒŒì¼: /Volumes/SAMSUNG/apps/projects/vive-md/docs/ontology-papers/originals/58_a-short-review-for-ontology-learning-stride-to-large-language-models-trend.pdf
- ë²ˆì—­ ìƒíƒœ: partial_translated

## ì›ë¬¸(ì¶”ì¶œ í…ìŠ¤íŠ¸)

A S HORT REVIEW FOR ONTOLOGY LEARNING : S TRIDE TO
LARGE LANGUAGE MODELS TREND
Rick Du, Huilong An, Keyu Wang
BSH Home Appliances Holding (China) Co., Ltd
{Rick.Du, Huilong.An, Keyu.Wang}@bshg.com
Weidong Liu
Department of Computer Science and Technology, Tsinghua University
liuwd@tsinghua.edu.cn
ABSTRACT
Ontologies provide formal representation of knowledge shared within Semantic Web applications.
Ontology learning involves the construction of ontologies from a given corpus. In the past years,
ontology learning has traversed through shallow learning and deep learning methodologies, each
offering distinct advantages and limitations in the quest for knowledge extraction and representation.
A new trend of these approaches is relying on large language models (LLMs) to enhance ontology
learning. This paper gives a review in approaches and challenges of ontology learning. It analyzes
the methodologies and limitations of shallow-learning-based and deep-learning-based techniques for
ontology learning, and provides comprehensive knowledge for the frontier work of using LLMs to
enhance ontology learning. In addition, it proposes several noteworthy future directions for further
exploration into the integration of LLMs with ontology learning tasks.
1 Introduction
Extraction and organization of meaningful conceptual knowledge have been central to the pursuit of enhancing machine
comprehension and reasoning capabilities [1]. Ontology learning, a fundamental cornerstone within this domain, is
tasked with the extraction, representation, and refinement of structured ontologies that encapsulate the intricacies of
various domains [2].
In the past years, ontology learning has traversed through shallow learning and deep learning methodologies, each
offering distinct advantages and limitations in the quest for knowledge extraction and representation [ 3]. Shallow
learning techniques, characterized by their simplicity and ease of implementation, have long been the bedrock of
ontology learning [4]. These methods, albeit effective in certain contexts, often grapple with challenges of scalability
and the extraction of nuanced and complex relationships between entities. Conversely, the advent of deep learning
techniques has heralded a new era, promising more intricate representations and enhanced discernment of underlying
patterns within data. However, deep learning techniques come burdened with their own set of limitations, including the
voracious appetite for large volumes of annotated data and computational resources [5].
Amidst this landscape, the emergence of large language models stands as a disruptive force, reshaping the contours
of ontology learning [6, 7]. These models, leveraging the prowess of pre-trained language representations, exhibit a
remarkable aptitude for understanding semantic nuances, capturing context, and inferring relationships among entities
[6, 7, 8, 9]. Their applications in ontology learning holds the promise of addressing longstanding challenges by
harnessing the inherent linguistic and conceptual understanding embedded within these models.
The purpose of this paper is to give a review in approaches and challenges of ontology learning in LLMs era. It presents
the methods and analyzes the limitations of shallow-learning-based and deep-learning-based techniques, and provides
comprehensive knowledge for the current work of using LLMs to enhance ontology learning. In addition, it proposes
several noteworthy future directions for further exploration into the integration of large language models with ontology
arXiv:2404.14991v2  [cs.IR]  17 Jun 2024

learning tasks. The rest of this paper is organized as follows: Section 2 defines ontology, ontology learning, and
summarises the challenges of ontology learning. Section 3 presents the ontology learning approaches based on shallow
learning and deep learning, as well as their limitations. Section 4 presents how large language models contributes to
ontology learning procedure recently, and discusses the potential of using large language models to facilitate ontology
learning. Section 5 proposes several future directions for further exploration into using large language models to
enhance ontology learning. Finally, we conclude in Section 6.
2 Ontology Learning
2.1 Ontology
In general, an ontology describes formally a domain of discourse. Typically, an ontology consists of terms and the
relationships between these terms, where the terms denote important concepts of the domain [10]. An ontology must
be formal and machine-readable, allowing it to serve as a shared vocabulary across different applications. Formally,
ontology can be described as following tuple [11]:
O =< C, H, R, A > (1)
where O represents ontology, C represents a set of classes (concepts), H represents a set of hierarchical links between
the concepts (taxonomic relations), R represents a set of conceptual links (non-taxonomic relations), and A represents a
set of rules and axioms.
2.2 Ontology Learning
Ontology learning (OL) from text involves the construction of ontologies from a given corpus of text [12, 13]. According
to ontology learning layer cake shown in Figure 1 proposed by [14], which is widely held as cornerstone in OL [15],
the process of OL from text can be divided into six sub-tasks as following:
Terms
Synonyms
Concepts
Concept Hierarchies
Relations
Rules
disease, illness, hospital
{disease, illness}
DISEASE â‰”< ğ¼, ğ¸, ğ¿ >
ğ‘–ğ‘ _ğ‘(ğ·ğ‘‚ğ¶ğ‘‡ğ‘‚ğ‘…, ğ‘ƒğ¸ğ‘…ğ‘†ğ‘‚ğ‘)
ğ‘ğ‘¢ğ‘Ÿğ‘’(ğ‘‘ğ‘œğ‘š: ğ·ğ‘‚ğ¶ğ‘‡ğ‘‚ğ‘…, ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’: ğ‘ƒğ¸ğ‘…ğ‘†ğ‘‚ğ‘)
âˆ€ğ‘¥, ğ‘¦(ğ‘šğ‘ğ‘Ÿğ‘Ÿğ‘–ğ‘’ğ‘‘ ğ‘¥, ğ‘¦ â†’ ğ‘™ğ‘œğ‘£ğ‘’(ğ‘¥, ğ‘¦)
Figure 1: Ontology Learning Layer Cake [14]
1. Term extraction: This initial step involves identifying relevant terms or entities from a given text or dataset.
These terms serve as the building blocks for constructing an ontology.
2. Synonym extraction: Synonyms are different terms referring to the same concept. In ontology learning,
identifying synonyms is crucial for ensuring comprehensive coverage and avoiding redundancy.
3. Concept formation: Once terms and their synonyms are extracted, the next step is to group them into
meaningful concepts or classes. This involves organizing related terms into hierarchies or categories based on
their similarities, functionalities, or semantic relations.
2

4. Taxonomic relation extraction: Taxonomic relations establish hierarchical relationships between concepts,
defining the "is-a" relationship (e.g., "car" is a "vehicle"). Ontology learning involves identifying and
structuring these hierarchical relationships to arrange concepts in a taxonomy or ontology hierarchy.
5. Non-taxonomic relation extraction: Unlike taxonomic relations, non-taxonomic relations capture various
associations between concepts beyond hierarchical relationships. These relations could be "part-of," "has-
property," or other associative connections that enrich the ontologyâ€™s expressiveness.
6. Rule or axiom extraction: Rules or axioms define constraints, dependencies, or logical relationships between
entities or concepts in the ontology. Extracting rules or axioms aims to formalize domain knowledge and
establish logical constraints within the ontology.
Generally, the ontology learning process follows the aforementioned steps. However, it is not uncommon for some
ontology learning processes only partially complete the six steps outlined above according to different needs. Ontology
learning methods can be roughly divided into the following three categories [16, 17, 18, 19]:
â€¢ Manual: Ontologies are developed through a process that heavily relies on human expertise and intervention.
Examples are Gene Ontology (GO) [ 20], WordNet [ 21], SNOMED CT (Systematized Nomenclature of
Medicineâ€”Clinical Terms) [22], Cyc [23], and Foundational Model of Anatomy (FMA) [24].
â€¢ Semi-automatic: The development of ontologies is facilitated and streamlined by integrating automated
processes with human input. There are various available tools for such a purpose, like Text2Onto [ 25],
OntoGen [26], and OntoStudio [27].
â€¢ Fully automatic: The system takes care of the complete construction, without any manual intervention. While
the idea of fully automatic ontology construction is appealing, especially for handling large volumes of data or
complex domains, it is worth mentioning that full automatic construction for ontology by a system is still a
significant challenge and it is not likely to be possible [28, 29, 30].
2.3 Challenges in Ontology Learning
Ontology learning, despite its advancements, still encounters various challenges. Below is a list highlighting the key
aspects that characterize the primary challenges in ontology learning:
Labor intensiveness: Ontology construction often involves significant manual effort. Identifying, extracting, and
structuring knowledge from diverse sources demands extensive human intervention. This labor-intensive process
can be time-consuming and resource-intensive, hindering the scalability and efficiency of ontology development
[15, 31, 32, 14].
Axiom formulation: Formulating precise axioms or rules that accurately represent domain knowledge poses a
challenge. Balancing expressiveness with computational efficiency is crucial. Axioms must be meaningful and precise
to contribute effectively to the ontologyâ€™s utility. This demands specialized expertise and often involves iterative
refinement [33, 31, 32].
Domain-specific knowledge acquisition: Acquiring and representing domain-specific knowledge within the ontology
is challenging. Understanding and capturing intricate domain nuances, concepts, and relationships require expert
domain knowledge. Incorporating evolving or specialized domain terminologies into the ontology accurately is complex
[34, 35].
Dynamic environments: Adapting ontologies to dynamic or evolving environments is challenging. Ensuring ontology
coherence and consistency while accommodating changes in domain concepts, terminologies, or relationships demands
continuous updates and version control mechanisms [36, 37].
Ambiguity and uncertainty: Dealing with ambiguous terms, uncertain knowledge representations, or conflicting
information within data sources presents challenges. Resolving ambiguity and handling uncertain or conflicting data
affect the ontologyâ€™s accuracy and reliability [31, 32].
Scalability: Ontology learning must accommodate large-scale data and knowledge sources while maintaining compu-
tational efficiency. Scaling ontology construction methods to handle substantial volumes of data without sacrificing
accuracy remains a significant challenge [38, 39].
Heterogeneity of data: Integrating heterogeneous data from diverse sources, each with different structures, formats,
and semantics, presents challenges. Aligning and reconciling conflicting data representations and resolving semantic
mismatches is crucial for creating coherent and comprehensive ontologies [14, 33, 31, 32].
3

Evaluation and validation: Properly evaluating ontologies for accuracy, completeness, and usability is complex.
Defining reliable evaluation metrics, validation methods, and assessing ontology quality pose challenges due to the
subjective nature of evaluating knowledge representations [31, 32, 40, 14].
3 Ontology Learning Approaches
3.1 Shallow-learning-based Approaches
Before the rise of deep learning, shallow learning methods grounded in traditional machine learning and classical neural
networks was predominant in ontology learning tasks such as term extraction, concept formation, taxonomy discovery,
non-taxonomic relation extraction, and axiom extraction [3]. These techniques mainly fall into three categories [3, 41]:
â€¢ Linguistics-based approaches. Linguistic techniques are based on characteristics of language, such as pattern-
based extraction [42], POS tagging and sentence parsing [43], syntactic structure analysis and dependency
structure analysis [44, 45] and etc.
â€¢ Statistics-based approaches. Statistical techniques are based on statistics of the underlying corpora. Typical
methods include co-occurrence analysis [ 46],

## í•œêµ­ì–´ ë²ˆì—­

ì˜¨í†¨ë¡œì§€ í•™ìŠµì— ëŒ€í•œ ê°„ëµí•œ ê²€í† : S TRIDE TO
ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ë™í–¥
Rick Du, Huilong An, Keyu Wang
BSH Home Appliances Holding (China) Co., Ltd.
{Rick.Du, Huilong.An, Keyu.Wang}@bshg.com
ë¦¬ìš° ì›¨ì´ë™
ì²­í™”ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼
liuwd@tsinghua.edu.cn
ê°œìš”
ì˜¨í†¨ë¡œì§€ëŠ” ì‹œë§¨í‹± ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ë‚´ì—ì„œ ê³µìœ ë˜ëŠ” ì§€ì‹ì˜ ê³µì‹ì ì¸ í‘œí˜„ì„ ì œê³µí•©ë‹ˆë‹¤.
ì˜¨í†¨ë¡œì§€ í•™ìŠµì—ëŠ” ì£¼ì–´ì§„ ì½”í¼ìŠ¤ë¡œë¶€í„° ì˜¨í†¨ë¡œì§€ë¥¼ êµ¬ì„±í•˜ëŠ” ì‘ì—…ì´ í¬í•¨ë©ë‹ˆë‹¤. ì§€ë‚œ ëª‡ ë…„ ë™ì•ˆ,
ì˜¨í†¨ë¡œì§€ í•™ìŠµì€ ì–•ì€ í•™ìŠµê³¼ ê¹Šì€ í•™ìŠµ ë°©ë²•ë¡ ì„ ê±°ì³ ì™”ìŠµë‹ˆë‹¤.
ì§€ì‹ ì¶”ì¶œ ë° í‘œí˜„ì„ ì¶”êµ¬í•˜ëŠ” ë° ìˆì–´ ëšœë ·í•œ ì¥ì ê³¼ í•œê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì˜ ìƒˆë¡œìš´ ì¶”ì„¸ëŠ” ì˜¨í†¨ë¡œì§€ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì— ì˜ì¡´í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
í•™ìŠµ. ì´ ë…¼ë¬¸ì€ ì˜¨í†¨ë¡œì§€ í•™ìŠµì˜ ì ‘ê·¼ ë°©ì‹ê³¼ ê³¼ì œì— ëŒ€í•œ ê²€í† ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë¶„ì„í•œë‹¤
ì–•ì€ í•™ìŠµ ê¸°ë°˜ ê¸°ìˆ ê³¼ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê¸°ìˆ ì˜ ë°©ë²•ë¡ ê³¼ í•œê³„
ì˜¨í†¨ë¡œì§€ í•™ìŠµì„ í†µí•´ LLMì„ ì‚¬ìš©í•˜ëŠ” ìµœì „ì„  ì‘ì—…ì— ëŒ€í•œ í¬ê´„ì ì¸ ì§€ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.
ì˜¨í†¨ë¡œì§€ í•™ìŠµì„ ê°•í™”í•©ë‹ˆë‹¤. ë˜í•œ, í–¥í›„ ëª‡ ê°€ì§€ ì£¼ëª©í• ë§Œí•œ ë¯¸ë˜ ë°©í–¥ì„ ì œì•ˆí•©ë‹ˆë‹¤.
ì˜¨í†¨ë¡œì§€ í•™ìŠµ ì‘ì—…ê³¼ LLMì˜ í†µí•©ì— ëŒ€í•œ íƒêµ¬.
1 ì†Œê°œ
ì˜ë¯¸ ìˆëŠ” ê°œë… ì§€ì‹ì˜ ì¶”ì¶œê³¼ ì¡°ì§ì€ ê¸°ê³„ í–¥ìƒì„ ì¶”êµ¬í•˜ëŠ” ë° í•µì‹¬ì´ì—ˆìŠµë‹ˆë‹¤.
ì´í•´ë ¥ê³¼ ì¶”ë¡ ë ¥ [1] ì´ ì˜ì—­ì˜ ê·¼ë³¸ì ì¸ ì´ˆì„ì¸ ì˜¨í†¨ë¡œì§€ í•™ìŠµì€
ë³µì¡ì„±ì„ ìº¡ìŠí™”í•˜ëŠ” êµ¬ì¡°í™”ëœ ì˜¨í†¨ë¡œì§€ì˜ ì¶”ì¶œ, í‘œí˜„ ë° ê°œì„ ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
ë‹¤ì–‘í•œ ë„ë©”ì¸ [2].
ì§€ë‚œ ëª‡ ë…„ ë™ì•ˆ ì˜¨í†¨ë¡œì§€ í•™ìŠµì€ ì–•ì€ í•™ìŠµê³¼ ê¹Šì€ í•™ìŠµ ë°©ë²•ë¡ ì„ ê±°ì³ ì™”ìŠµë‹ˆë‹¤.
ì§€ì‹ ì¶”ì¶œ ë° í‘œí˜„ì„ ì¶”êµ¬í•˜ëŠ” ë° ìˆì–´ ëšœë ·í•œ ì¥ì ê³¼ í•œê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤[3]. ì–•ì€
ë‹¨ìˆœì„±ê³¼ êµ¬í˜„ ìš©ì´ì„±ì„ íŠ¹ì§•ìœ¼ë¡œ í•˜ëŠ” í•™ìŠµ ê¸°ìˆ ì€ ì˜¤ë«ë™ì•ˆ êµìœ¡ì˜ ê¸°ë°˜ì´ ë˜ì–´ ì™”ìŠµë‹ˆë‹¤.
ì˜¨í†¨ë¡œì§€ í•™ìŠµ [4]. ì´ëŸ¬í•œ ë°©ë²•ì€ íŠ¹ì • ìƒí™©ì—ì„œëŠ” íš¨ê³¼ì ì´ì§€ë§Œ ì¢…ì¢… í™•ì¥ì„± ë¬¸ì œë¡œ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.
ì—”í„°í‹° ê°„ì˜ ë¯¸ë¬˜í•˜ê³  ë³µì¡í•œ ê´€ê³„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ ë”¥ëŸ¬ë‹ì˜ ë“±ì¥
ê¸°ìˆ ì€ ë³´ë‹¤ ë³µì¡í•œ í‘œí˜„ê³¼ ê¸°ë³¸ ìš”ì†Œì— ëŒ€í•œ í–¥ìƒëœ ì‹ë³„ë ¥ì„ ì•½ì†í•˜ë©´ì„œ ìƒˆë¡œìš´ ì‹œëŒ€ë¥¼ ì˜ˆê³ í–ˆìŠµë‹ˆë‹¤.
ë°ì´í„° ë‚´ì˜ íŒ¨í„´. ê·¸ëŸ¬ë‚˜ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
ì£¼ì„ì´ ë‹¬ë¦° ëŒ€ëŸ‰ì˜ ë°ì´í„°ì™€ ê³„ì‚° ë¦¬ì†ŒìŠ¤ì— ëŒ€í•œ íƒìš•ìŠ¤ëŸ¬ìš´ ìš•êµ¬.
ì´ëŸ¬í•œ í™˜ê²½ ì†ì—ì„œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì¶œí˜„ì€ ìœ¤ê³½ì„ ì¬í˜•ì„±í•˜ëŠ” íŒŒê´´ì ì¸ í˜ìœ¼ë¡œ ì‘ìš©í•©ë‹ˆë‹¤.
ì˜¨í†¨ë¡œì§€ í•™ìŠµ [6, 7]. ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ í‘œí˜„ì˜ ëŠ¥ë ¥ì„ í™œìš©í•˜ëŠ” ì´ëŸ¬í•œ ëª¨ë¸ì€
ì˜ë¯¸ë¡ ì  ë‰˜ì•™ìŠ¤ë¥¼ ì´í•´í•˜ê³ , ë§¥ë½ì„ íŒŒì•…í•˜ê³ , ê°œì²´ ê°„ì˜ ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ë†€ë¼ìš´ ëŠ¥ë ¥
[6, 7, 8, 9]. ì˜¨í†¨ë¡œì§€ í•™ìŠµì— ëŒ€í•œ ê·¸ë“¤ì˜ ì ìš©ì€
ì´ëŸ¬í•œ ëª¨ë¸ì— ë‚´ì¬ëœ ê³ ìœ í•œ ì–¸ì–´ì , ê°œë…ì  ì´í•´ë¥¼ í™œìš©í•©ë‹ˆë‹¤.
ì´ ë…¼ë¬¸ì˜ ëª©ì ì€ LLM ì‹œëŒ€ì˜ ì˜¨í†¨ë¡œì§€ í•™ìŠµì— ëŒ€í•œ ì ‘ê·¼ ë°©ì‹ê³¼ ê³¼ì œë¥¼ ê²€í† í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ê²ƒì€ ì„ ë¬¼í•œë‹¤
ì–•ì€ í•™ìŠµ ê¸°ë°˜ê³¼ ë”¥ ëŸ¬ë‹ ê¸°ë°˜ ê¸°ìˆ ì˜ í•œê³„ì ì„ ë¶„ì„í•˜ê³  ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.
ì˜¨í†¨ë¡œì§€ í•™ìŠµì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ LLMì„ ì‚¬ìš©í•˜ëŠ” í˜„ì¬ ì‘ì—…ì— ëŒ€í•œ í¬ê´„ì ì¸ ì§€ì‹. ê²Œë‹¤ê°€ ì œì•ˆí•œë‹¤.
ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ê³¼ ì˜¨í†¨ë¡œì§€ì˜ í†µí•©ì— ëŒ€í•œ ì¶”ê°€ íƒêµ¬ë¥¼ ìœ„í•œ ëª‡ ê°€ì§€ ì£¼ëª©í• ë§Œí•œ ë¯¸ë˜ ë°©í–¥
arXiv:2404.14991v2 [cs.IR] 2024ë…„ 6ì›” 17ì¼

í•™ìŠµ ê³¼ì œ. ë³¸ ë…¼ë¬¸ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë©ë‹ˆë‹¤. 2ì¥ì—ì„œëŠ” ì˜¨í†¨ë¡œì§€, ì˜¨í†¨ë¡œì§€ í•™ìŠµ ë°
ì˜¨í†¨ë¡œì§€ í•™ìŠµì˜ ê³¼ì œë¥¼ ìš”ì•½í•©ë‹ˆë‹¤. ì„¹ì…˜ 3ì—ì„œëŠ” ì–•ì€ ê¸°ë°˜ì˜ ì˜¨í†¨ë¡œì§€ í•™ìŠµ ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.
í•™ìŠµê³¼ ë”¥ëŸ¬ë‹, ê·¸ë¦¬ê³  ê·¸ í•œê³„. ì„¹ì…˜ 4ì—ì„œëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ ì–´ë–»ê²Œ ê¸°ì—¬í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.
ìµœê·¼ ì˜¨í†¨ë¡œì§€ í•™ìŠµ ì ˆì°¨ë¥¼ ë‹¤ë£¨ê³ , ì˜¨í†¨ë¡œì§€ë¥¼ ì´‰ì§„í•˜ê¸° ìœ„í•´ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•  ê°€ëŠ¥ì„±ì— ëŒ€í•´ ë…¼ì˜í•©ë‹ˆë‹¤.
í•™ìŠµ. ì„¹ì…˜ 5ì—ì„œëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ íƒìƒ‰ì„ ìœ„í•œ ëª‡ ê°€ì§€ í–¥í›„ ë°©í–¥ì„ ì œì•ˆí•©ë‹ˆë‹¤.
ì˜¨í†¨ë¡œì§€ í•™ìŠµì„ ê°•í™”í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ 6ì¥ì—ì„œ ê²°ë¡ ì„ ë§ºëŠ”ë‹¤.
2 ì˜¨í†¨ë¡œì§€ í•™ìŠµ
2.1 ì˜¨í†¨ë¡œì§€
ì¼ë°˜ì ìœ¼ë¡œ ì˜¨í†¨ë¡œì§€ëŠ” ë‹´ë¡ ì˜ ì˜ì—­ì„ í˜•ì‹ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì˜¨í†¨ë¡œì§€ëŠ” ìš©ì–´ì™€
ì—¬ê¸°ì„œ ìš©ì–´ëŠ” ë„ë©”ì¸ì˜ ì¤‘ìš”í•œ ê°œë…ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤ [10]. ì˜¨í†¨ë¡œì§€ëŠ” ë°˜ë“œì‹œ
í˜•ì‹ì ì´ê³  ê¸°ê³„ íŒë…ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ê³µìœ  ì–´íœ˜ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³µì‹ì ìœ¼ë¡œ,
ì˜¨í†¨ë¡œì§€ëŠ” ë‹¤ìŒ íŠœí”Œ [11]ë¡œ ì„¤ëª…ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
O =< C, H, R, A > (1)
ì—¬ê¸°ì„œ OëŠ” ì˜¨í†¨ë¡œì§€ë¥¼ ë‚˜íƒ€ë‚´ê³ , CëŠ” í´ë˜ìŠ¤(ê°œë…) ì§‘í•©ì„ ë‚˜íƒ€ë‚´ë©°, HëŠ” í´ë˜ìŠ¤ ê°„ì˜ ê³„ì¸µì  ë§í¬ ì§‘í•©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
ê°œë…(ë¶„ë¥˜í•™ì  ê´€ê³„), Rì€ ì¼ë ¨ì˜ ê°œë…ì  ë§í¬(ë¹„ë¶„ë¥˜í•™ì  ê´€ê³„)ë¥¼ ë‚˜íƒ€ë‚´ê³ , AëŠ”
ì¼ë ¨ì˜ ê·œì¹™ê³¼ ê³µë¦¬.
2.2 ì˜¨í†¨ë¡œì§€ í•™ìŠµ
í…ìŠ¤íŠ¸ë¡œë¶€í„° ì˜¨í†¨ë¡œì§€ í•™ìŠµ(OL)ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ë¡œë¶€í„° ì˜¨í†¨ë¡œì§€ë¥¼ êµ¬ì„±í•˜ëŠ” ê²ƒì„ í¬í•¨í•©ë‹ˆë‹¤[12, 13]. ë”°ë¥´ë©´
OL [15]ì˜ ì´ˆì„ìœ¼ë¡œ ë„ë¦¬ ë°›ì•„ë“¤ì—¬ì§€ê³  ìˆëŠ” [14]ì—ì„œ ì œì•ˆí•œ ê·¸ë¦¼ 1ì˜ ì˜¨í†¨ë¡œì§€ í•™ìŠµ ë ˆì´ì–´ ì¼€ì´í¬ì—,
í…ìŠ¤íŠ¸ì—ì„œ OLì„ ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ 6ê°€ì§€ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ìš©ì–´
ë™ì˜ì–´
ê°œë…
ê°œë… ê³„ì¸µ
ê´€ê³„
ê·œì¹™
ì§ˆë³‘, ë³‘, ë³‘ì›
{ì§ˆë³‘, ì§ˆë³‘}
ì§ˆë³‘ â‰”< ğ¼, ğ¸, ğ¿ >
ğ‘–ğ‘ _ğ‘(ğ·ğ‘‚ğ¶ğ‘‡ğ‘‚ğ‘…, ğ‘ƒğ¸ğ‘…ğ‘†ğ‘‚ğ‘)
ğ‘ğ‘¢ğ‘Ÿğ‘’(ğ‘‘ğ‘œğ‘š: ğ·ğ‘‚ğ¶ğ‘‡ğ‘‚ğ‘…, ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’: ğ‘ƒğ¸ğ‘…ğ‘†ğ‘‚ğ‘)
âˆ€ğ‘¥, ğ‘¦(ğ‘šğ‘ğ‘Ÿğ‘Ÿğ‘–ğ‘’ğ‘‘ ğ‘¥, ğ‘¦ â†’ ğ‘™ğ‘œğ‘£ğ‘’(ğ‘¥, ğ‘¦)
ê·¸ë¦¼ 1: ì˜¨í†¨ë¡œì§€ í•™ìŠµ ë ˆì´ì–´ ì¼€ì´í¬ [14]
1. ìš©ì–´ ì¶”ì¶œ: ì´ ì´ˆê¸° ë‹¨ê³„ì—ëŠ” ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë‚˜ ë°ì´í„°ì„¸íŠ¸ì—ì„œ ê´€ë ¨ ìš©ì–´ë‚˜ ê°œì²´ë¥¼ ì‹ë³„í•˜ëŠ” ì‘ì—…ì´ í¬í•¨ë©ë‹ˆë‹¤.
ì´ëŸ¬í•œ ìš©ì–´ëŠ” ì˜¨í†¨ë¡œì§€ë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•œ êµ¬ì„± ìš”ì†Œ ì—­í• ì„ í•©ë‹ˆë‹¤.
2. ë™ì˜ì–´ ì¶”ì¶œ: ë™ì˜ì–´ëŠ” ë™ì¼í•œ ê°œë…ì„ ê°€ë¦¬í‚¤ëŠ” ë‹¤ë¥¸ ìš©ì–´ì…ë‹ˆë‹¤. ì˜¨í†¨ë¡œì§€ í•™ìŠµì—ì„œëŠ”
í¬ê´„ì ì¸ ì ìš© ë²”ìœ„ë¥¼ ë³´ì¥í•˜ê³  ì¤‘ë³µì„ ë°©ì§€í•˜ë ¤ë©´ ë™ì˜ì–´ë¥¼ ì‹ë³„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
3. ê°œë… í˜•ì„±: ìš©ì–´ ë° ë™ì˜ì–´ê°€ ì¶”ì¶œë˜ë©´ ë‹¤ìŒ ë‹¨ê³„ëŠ” ì´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê·¸ë£¹í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ì˜ë¯¸ ìˆëŠ” ê°œë…ì´ë‚˜ ìˆ˜ì—…. ì—¬ê¸°ì—ëŠ” ê´€ë ¨ ìš©ì–´ë¥¼ ë‹¤ìŒì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì¸µ êµ¬ì¡° ë˜ëŠ” ë²”ì£¼ë¡œ êµ¬ì„±í•˜ëŠ” ì‘ì—…ì´ í¬í•¨ë©ë‹ˆë‹¤.
ìœ ì‚¬ì„±, ê¸°ëŠ¥ ë˜ëŠ” ì˜ë¯¸ë¡ ì  ê´€ê³„.
2

4. ë¶„ë¥˜í•™ì  ê´€ê³„ ì¶”ì¶œ: ë¶„ë¥˜í•™ì  ê´€ê³„ëŠ” ê°œë… ê°„ì˜ ê³„ì¸µì  ê´€ê³„ë¥¼ ì„¤ì •í•˜ê³ ,
"is-a" ê´€ê³„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤(ì˜ˆ: "car"ëŠ” "vehicle"ì…ë‹ˆë‹¤). ì˜¨í†¨ë¡œì§€ í•™ìŠµì—ëŠ” ì‹ë³„ê³¼
ë¶„ë¥˜ë²• ë˜ëŠ” ì˜¨í†¨ë¡œì§€ ê³„ì¸µ êµ¬ì¡°ì—ì„œ ê°œë…ì„ ë°°ì—´í•˜ê¸° ìœ„í•´ ì´ëŸ¬í•œ ê³„ì¸µì  ê´€ê³„ë¥¼ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.
5. ë¹„ë¶„ë¥˜í•™ì  ê´€ê³„ ì¶”ì¶œ: ë¶„ë¥˜í•™ì  ê´€ê³„ì™€ ë‹¬ë¦¬ ë¹„ë¶„ë¥˜í•™ì  ê´€ê³„ëŠ” ë‹¤ì–‘í•œ
ê³„ì¸µì  ê´€ê³„ë¥¼ ë„˜ì–´ì„œëŠ” ê°œë… ê°„ì˜ ì—°ê´€. ì´ëŸ¬í•œ ê´€ê³„ëŠ” "~ì˜ ì¼ë¶€", "~"ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì†ì„±" ë˜ëŠ” ì˜¨í†¨ë¡œì§€ì˜ í‘œí˜„ë ¥ì„ í’ë¶€í•˜ê²Œ í•˜ëŠ” ê¸°íƒ€ ì—°ê´€ ì—°ê²°.
6. ê·œì¹™ ë˜ëŠ” ê³µë¦¬ ì¶”ì¶œ: ê·œì¹™ ë˜ëŠ” ê³µë¦¬ëŠ” ë‘˜ ì‚¬ì´ì˜ ì œì•½, ì¢…ì†ì„± ë˜ëŠ” ë…¼ë¦¬ì  ê´€ê³„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
ì˜¨í†¨ë¡œì§€ì˜ ì—”í„°í‹° ë˜ëŠ” ê°œë…. ê·œì¹™ì´ë‚˜ ê³µë¦¬ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì€ ë„ë©”ì¸ ì§€ì‹ì„ ê³µì‹í™”í•˜ê³ 
ì˜¨í†¨ë¡œì§€ ë‚´ì—ì„œ ë…¼ë¦¬ì  ì œì•½ ì¡°ê±´ì„ ì„¤ì •í•©ë‹ˆë‹¤.
ì¼ë°˜ì ìœ¼ë¡œ ì˜¨í†¨ë¡œì§€ í•™ìŠµ ê³¼ì •ì€ ì•ì„œ ì–¸ê¸‰í•œ ë‹¨ê³„ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¼ë¶€ì—ê²ŒëŠ” ë“œë¬¸ ì¼ì´ ì•„ë‹™ë‹ˆë‹¤.
ì˜¨í†¨ë¡œì§€ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ì–‘í•œ ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ ìœ„ì— ì„¤ëª…ëœ 6ë‹¨ê³„ë¥¼ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ì™„ë£Œí•©ë‹ˆë‹¤. ì˜¨í†¨ë¡œì§€
í•™ìŠµ ë°©ë²•ì€ ëŒ€ëµ ë‹¤ìŒ ì„¸ ê°€ì§€ ë²”ì£¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤ [16, 17, 18, 19].
â€¢ ìˆ˜ë™: ì˜¨í†¨ë¡œì§€ëŠ” ì¸ê°„ì˜ ì „ë¬¸ ì§€ì‹ê³¼ ê°œì…ì— í¬ê²Œ ì˜ì¡´í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ê°œë°œë©ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ë©´ Gene Ontology(GO) [ 20], WordNet [ 21], SNOMED CT(Systematized Nomenclature of
ì˜í•™ - ì„ìƒ ìš©ì–´) [22], Cyc [23] ë° FMA(Foundational Model of Anatomy) [24].
â€¢ ë°˜ìë™: ìë™í™”ëœ ê¸°ëŠ¥ì„ í†µí•©í•˜ì—¬ ì˜¨í†¨ë¡œì§€ ê°œë°œì„ ì´‰ì§„í•˜ê³  ê°„ì†Œí™”í•©ë‹ˆë‹¤.
ì¸ê°„ì˜ ì…ë ¥ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª©ì ì„ ìœ„í•´ Text2Onto [ 25]ì™€ ê°™ì€ ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
OntoGen [26] ë° OntoStudio [27].
â€¢ ì™„ì „ ìë™: ìˆ˜ë™ ê°œì… ì—†ì´ ì‹œìŠ¤í…œì´ ì „ì²´ êµ¬ì„±ì„ ê´€ë¦¬í•©ë‹ˆë‹¤. ë™ì•ˆ
ì™„ì „ ìë™ ì˜¨í†¨ë¡œì§€ êµ¬ì¶• ì•„ì´ë””ì–´ëŠ” íŠ¹íˆ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜
ë³µì¡í•œ ë„ë©”ì¸ì—ì„œëŠ” ì‹œìŠ¤í…œì— ì˜í•œ ì˜¨í†¨ë¡œì§€ë¥¼ ìœ„í•œ ì™„ì „ ìë™ êµ¬ì„±ì´ ì—¬ì „íˆ
ì¤‘ëŒ€í•œ ë„ì „ì´ë©° ë¶ˆê°€ëŠ¥í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤ [28, 29, 30].
2.3 ì˜¨í†¨ë¡œì§€ í•™ìŠµì˜ ê³¼ì œ
ì˜¨í†¨ë¡œì§€ í•™ìŠµì€ ë°œì „ì—ë„ ë¶ˆêµ¬í•˜ê³  ì—¬ì „íˆ ë‹¤ì–‘í•œ ê³¼ì œì— ì§ë©´í•´ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” í•µì‹¬ì„ ê°•ì¡°í•œ ëª©ë¡ì…ë‹ˆë‹¤.
ì˜¨í†¨ë¡œì§€ í•™ìŠµì˜ ì£¼ìš” ê³¼ì œë¥¼ íŠ¹ì§•ì§“ëŠ” ì¸¡ë©´:
ë…¸ë™ ì§‘ì•½ì„±: ì˜¨í†¨ë¡œì§€ êµ¬ì¶•ì—ëŠ” ì¢…ì¢… ìƒë‹¹í•œ ìˆ˜ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤. ì‹ë³„, ì¶”ì¶œ ë°
ë‹¤ì–‘í•œ ì¶œì²˜ì˜ ì§€ì‹ì„ êµ¬ì¡°í™”í•˜ë ¤ë©´ ê´‘ë²”ìœ„í•œ ì¸ê°„ ê°œì…ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ ë…¸ë™ì§‘ì•½ì ì¸ ê³¼ì •ì€
ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ê³  ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì ì´ì–´ì„œ ì˜¨í†¨ë¡œì§€ ê°œë°œì˜ í™•ì¥ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë°©í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
[15, 31, 32, 14].
ê³µë¦¬ ê³µì‹í™”: ë„ë©”ì¸ ì§€ì‹ì„ ì •í™•í•˜ê²Œ í‘œí˜„í•˜ëŠ” ì •í™•í•œ ê³µë¦¬ ë˜ëŠ” ê·œì¹™ì„ ê³µì‹í™”í•˜ëŠ” ê²ƒì€
ë„ì „. í‘œí˜„ë ¥ê³¼ ê³„ì‚° íš¨ìœ¨ì„±ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ê³µë¦¬ëŠ” ì˜ë¯¸ ìˆê³  ì •í™•í•´ì•¼ í•©ë‹ˆë‹¤.
ì˜¨í†¨ë¡œì§€ì˜ ìœ ìš©ì„±ì— íš¨ê³¼ì ìœ¼ë¡œ ê¸°ì—¬í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ì „ë¬¸ì ì¸ ì „ë¬¸ ì§€ì‹ì´ í•„ìš”í•˜ë©° ì¢…ì¢… ë°˜ë³µì ì¸ ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.
ê°œì„  [33, 31, 32].
ë„ë©”ì¸ íŠ¹ì • ì§€ì‹ íšë“: ì˜¨í†¨ë¡œì§€ ë‚´ì—ì„œ ë„ë©”ì¸ íŠ¹ì • ì§€ì‹ì„ íšë“í•˜ê³  í‘œí˜„í•©ë‹ˆë‹¤.
ë„ì „ì ì´ë‹¤. ë³µì¡í•œ ë„ë©”ì¸ ë‰˜ì•™ìŠ¤, ê°œë… ë° ê´€ê³„ë¥¼ ì´í•´í•˜ê³  í¬ì°©í•˜ë ¤ë©´ ì „ë¬¸ê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤.
ë„ë©”ì¸ ì§€ì‹. ì§„í™”í•˜ê±°ë‚˜ ì „ë¬¸í™”ëœ ë„ë©”ì¸ ìš©ì–´ë¥¼ ì˜¨í†¨ë¡œì§€ì— ì •í™•í•˜ê²Œ í†µí•©í•˜ëŠ” ê²ƒì€ ë³µì¡í•©ë‹ˆë‹¤.
[34, 35].
ë™ì  í™˜ê²½: ë™ì ì´ê±°ë‚˜ ì§„í™”í•˜ëŠ” í™˜ê²½ì— ì˜¨í†¨ë¡œì§€ë¥¼ ì ìš©í•˜ëŠ” ê²ƒì€ ì–´ë µìŠµë‹ˆë‹¤. ì˜¨í†¨ë¡œì§€ ë³´ì¥
ì˜ì—­ ê°œë…, ìš©ì–´ ë˜ëŠ” ê´€ê³„ ìš”êµ¬ ì‚¬í•­ì˜ ë³€í™”ë¥¼ ìˆ˜ìš©í•˜ë©´ì„œ ì¼ê´€ì„±ê³¼ ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
ì§€ì†ì ì¸ ì—…ë°ì´íŠ¸ ë° ë²„ì „ ì œì–´ ë©”ì»¤ë‹ˆì¦˜ [36, 37].
ëª¨í˜¸í•¨ê³¼ ë¶ˆí™•ì‹¤ì„±: ëª¨í˜¸í•œ ìš©ì–´, ë¶ˆí™•ì‹¤í•œ ì§€ì‹ í‘œí˜„ ë˜ëŠ” ìƒì¶©ë˜ëŠ” ë‚´ìš© ì²˜ë¦¬
ë°ì´í„° ì†ŒìŠ¤ ë‚´ì˜ ì •ë³´ëŠ” ë¬¸ì œë¥¼ ì•¼ê¸°í•©ë‹ˆë‹¤. ëª¨í˜¸í•¨ì„ í•´ê²°í•˜ê³  ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ìƒì¶©ë˜ëŠ” ë°ì´í„° ì²˜ë¦¬
ì˜¨í†¨ë¡œì§€ì˜ ì •í™•ì„±ê³¼ ì‹ ë¢°ì„±ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤ [31, 32].
í™•ì¥ì„±: ì˜¨í†¨ë¡œì§€ í•™ìŠµì€ ì»´í“¨íŒ…ì„ ìœ ì§€í•˜ë©´ì„œ ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ì§€ì‹ ì†ŒìŠ¤ë¥¼ ìˆ˜ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
êµ­ë ¥ íš¨ìœ¨ì„±. í¬ìƒ ì—†ì´ ìƒë‹¹í•œ ì–‘ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì˜¨í†¨ë¡œì§€ êµ¬ì¶• ë°©ë²• í™•ì¥
ì •í™•ì„±ì€ ì—¬ì „íˆ ì¤‘ìš”í•œ ê³¼ì œë¡œ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤[38, 39].
ë°ì´í„°ì˜ ì´ì§ˆì„±: ê°ê¸° ë‹¤ë¥¸ êµ¬ì¡°, í˜•ì‹,
ê·¸ë¦¬ê³  ì˜ë¯¸ë¡ ì€ ë„ì „ ê³¼ì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ìƒì¶©ë˜ëŠ” ë°ì´í„° í‘œí˜„ì„ ì •ë ¬ ë° ì¡°ì •í•˜ê³  ì˜ë¯¸ ì²´ê³„ë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
ë¶ˆì¼ì¹˜ëŠ” ì¼ê´€ë˜ê³  í¬ê´„ì ì¸ ì˜¨í†¨ë¡œì§€ë¥¼ ìƒì„±í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤ [14, 33, 31, 32].
3

í‰ê°€ ë° ê²€ì¦: ì˜¨í†¨ë¡œì§€ì˜ ì •í™•ì„±, ì™„ì „ì„± ë° ìœ ìš©ì„±ì„ ì ì ˆí•˜ê²Œ í‰ê°€í•˜ëŠ” ê²ƒì€ ë³µì¡í•©ë‹ˆë‹¤.
ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ ì§€í‘œ, ê²€ì¦ ë°©ë²•ì„ ì •ì˜í•˜ê³  ì˜¨í†¨ë¡œì§€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ê²ƒì€ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë¡œ ì¸í•´ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.
ì§€ì‹ í‘œí˜„ í‰ê°€ì˜ ì£¼ê´€ì  ì„±ê²© [31, 32, 40, 14].
3 ì˜¨í†¨ë¡œì§€ í•™ìŠµ ì ‘ê·¼ë²•
3.1 ì–•ì€ í•™ìŠµ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹
ë”¥ëŸ¬ë‹ì´ ë“±ì¥í•˜ê¸° ì „, ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ê³ ì „ì  ì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì–•ì€ í•™ìŠµ ë°©ë²•
ë„¤íŠ¸ì›Œí¬ëŠ” ìš©ì–´ ì¶”ì¶œ, ê°œë… í˜•ì„±, ë¶„ë¥˜ë²• ë°œê²¬,
ë¹„ë¶„ë¥˜í•™ì  ê´€ê³„ ì¶”ì¶œ, ê³µë¦¬ ì¶”ì¶œ [3]. ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì£¼ë¡œ ì„¸ ê°€ì§€ ë²”ì£¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤[3, 41].
â€¢ ì–¸ì–´í•™ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹. ì–¸ì–´í•™ì  ê¸°ë²•ì€ íŒ¨í„´-ì–¸ì–´ì™€ ê°™ì€ ì–¸ì–´ì˜ íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.
ê¸°ë°˜ ì¶”ì¶œ[42], POS íƒœê¹… ë° ë¬¸ì¥ íŒŒì‹±[43], êµ¬ë¬¸ êµ¬ì¡° ë¶„ì„ ë° ì¢…ì†ì„±
êµ¬ì¡° ë¶„ì„ [44, 45] ë“±
â€¢ í†µê³„ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹. í†µê³„ ê¸°ë²•ì€ ê¸°ë³¸ ë§ë­‰ì¹˜ì˜ í†µê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì „í˜•ì ì¸
ë°©ë²•ì—ëŠ” ë™ì‹œ ë°œìƒ ë¶„ì„ì´ í¬í•¨ë©ë‹ˆë‹¤ [ 46],
