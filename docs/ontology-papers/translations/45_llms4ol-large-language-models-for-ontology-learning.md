# [45] LLMs4OL: 온톨로지 학습을 위한 대규모 언어모델

- 영문 제목: LLMs4OL: Large Language Models for Ontology Learning
- 연도: 2023
- 원문 링크: https://doi.org/10.1007/978-3-031-47240-4_22
- DOI: 10.1007/978-3-031-47240-4_22
- 원문 저장 상태: html_saved
- 원문 파일: /Volumes/SAMSUNG/apps/projects/vive-md/docs/ontology-papers/originals/45_llms4ol-large-language-models-for-ontology-learning.html
- 번역 상태: partial_translated

## 원문(추출 텍스트)

LLMs4OL: Large Language Models for Ontology Learning | Springer Nature Link
Skip to main content
Advertisement
Log in
Menu
Find a journal
Publish with us
Track your research
Search
Saved research
Cart
Home
The Semantic Web – ISWC 2023
Conference paper
LLMs4OL: Large Language Models for Ontology Learning
Conference paper
First Online:
27 October 2023
pp 408–427
Cite this conference paper
The Semantic Web – ISWC 2023
(ISWC 2023)
Hamed Babaei Giglou
ORCID:
orcid.org/0000-0003-3758-1454
16
,
Jennifer D’Souza
ORCID:
orcid.org/0000-0002-6616-9509
16
&
Sören Auer
ORCID:
orcid.org/0000-0002-0698-2864
16
Part of the book series:
Lecture Notes in Computer Science
((LNCS,volume 14265))
Included in the following conference series:
International Semantic Web Conference
6198
Accesses
111
Citations
Abstract
We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis:
Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?
To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.
The obtained empirical results show that foundational LLMs are not sufficiently suitable for ontology construction that entails a high degree of reasoning skills and domain expertise. Nevertheless, when effectively fine-tuned they just might work as suitable assistants, alleviating the knowledge acquisition bottleneck, for ontology construction.
This is a preview of subscription content,
log in via an institution
to check access.
Access this chapter
Log in via an institution
Subscribe and save
Springer+
from €37.37 /Month
Starting from 10 chapters or articles per month
Access and download chapters and articles from more than 300k books and 2,500 journals
Cancel anytime
View plans
Buy Now
Chapter
EUR 29.95
Price includes VAT (Korea(Rep.))
Available as PDF
Read on any device
Instant download
Own it forever
Buy Chapter
eBook
EUR 71.68
Price includes VAT (Korea(Rep.))
Available as EPUB and PDF
Read on any device
Instant download
Own it forever
Buy eBook
Softcover Book
EUR 84.99
Price excludes VAT (Korea(Rep.))
Compact, lightweight edition
Dispatched in 3 to 5 business days
Free shipping worldwide -
see info
Buy Softcover Book
Tax calculation will be finalised at checkout
Purchases are for personal use only
Institutional subscriptions
Similar content being viewed by others
LLMs4OM: Matching Ontologies with Large Language Models
Chapter
© 2025
Do LLMs Really Adapt to Domains? An Ontology Learning Perspective
Chapter
© 2025
A Comprehensive Ontology Knowledge Evaluation System for Large Language Models
Chapter
© 2025
Explore related subjects
Discover the latest articles, books and news in related subjects, suggested using machine learning.
Analytical Philosophy of Language
Computational Linguistics
Ontology
Linear Logic
Natural Language Processing (NLP)
Research Methods in Language and Linguistics
Knowledge Graphs and Semantic Data Integration
References
Geonames geographical database (2023).
http://www.geonames.org/
Agirre, E., Ansa, O., Hovy, E., Martínez, D.: Enriching very large ontologies using the www. In: Proceedings of the First International Conference on Ontology Learning, vol. 31. pp. 25–30 (2000)
Google Scholar
Akkalyoncu Yilmaz, Z., Wang, S., Yang, W., Zhang, H., Lin, J.: Applying BERT to document retrieval with birch. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, pp. 19–24. Association for Computational Linguistics, Hong Kong, China (Nov 2019).
https://doi.org/10.18653/v1/D19-3004
,
https://aclanthology.org/D19-3004
Alfonseca, E., Manandhar, S.: An unsupervised method for general named entity recognition and automated concept discovery. In: Proceedings of the 1st International Conference on General WordNet, Mysore, India, pp. 34–43 (2002)
Google Scholar
Amatriain, X.: Transformer models: an introduction and catalog. arXiv preprint
arXiv:2302.07730
(2023)
Asim, M.N., Wasim, M., Khan, M.U.G., Mahmood, W., Abbasi, H.M.: A survey of ontology learning techniques and applications. Database 2018, bay101 (2018)
Google Scholar
Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., Ives, Z.: DBpedia: a nucleus for a web of open data. In: Aberer, K., et al. (eds.) ASWC/ISWC -2007. LNCS, vol. 4825, pp. 722–735. Springer, Heidelberg (2007).
https://doi.org/10.1007/978-3-540-76298-0_52
Chapter
Google Scholar
Bodenreider, O.: The unified medical language system (UMLS): integrating biomedical terminology. Nucleic Acids Res.
32
(suppl_1), D267–D270 2004).
https://doi.org/10.1093/nar/gkh061
Bodenreider, O.: The unified medical language system (umls): integrating biomedical terminology. Nucleic Acids Res.
32
(suppl_1), D267–D270 (2004)
Google Scholar
Brown, T.B., et al.: Language models are few-shot learners (2020)
Google Scholar
Chung, H.W., et al.: Scaling instruction-finetuned language models (2022)
Google Scholar
Cui, L., Wu, Y., Liu, J., Yang, S., Zhang, Y.: Template-based named entity recognition using bart. arXiv preprint
arXiv:2106.01760
(2021)
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Salakhutdinov, R.: Transformer-xl: attentive language models beyond a fixed-length context (2019)
Google Scholar
Dalvi, F., Khan, A.R., Alam, F., Durrani, N., Xu, J., Sajjad, H.: Discovering latent concepts learned in BERT. In: International Conference on Learning Representations (2022).
https://openreview.net/forum?id=POTMtpYI1xH
Dettmers, T., Pasquale, M., Pontus, S., Riedel, S.: Convolutional 2d knowledge graph embeddings. In: Proceedings of the 32th AAAI Conference on Artificial Intelligence, pp. 1811–1818 (February 2018),
https://arxiv.org/abs/1707.01476
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding (2019)
Google Scholar
Dopazo, J., Carazo, J.M.: Phylogenetic reconstruction using an unsupervised growing neural network that adopts the topology of a phylogenetic tree. J. Mol. Evol.
44
(2), 226–233 (1997)
Article
Google Scholar
Gruber, T.R.: Toward principles for the design of ontologies used for knowledge sharing? Int. J. Hum Comput Stud.
43
(5–6), 907–928 (1995)
Article
Google Scholar
Gu, Y., et al.: Domain-specific language model pretraining for biomedical natural language processing. ACM Trans. Comput. Healthcare (Health)
3
(1), 1–23 (2021)
Google Scholar
Guha, R.V., Brickley, D., Macbeth, S.: Schema. org: evolution of structured data on the web. Commun. ACM
59
(2), 44–51 (2016)
Google Scholar
Hahn, U., Markó, K.G.: Joint knowledge capture for grammars and ontologies. In: Proceedings of the 1st International Conference on Knowledge Capture, pp. 68–75 (2001)
Google Scholar
Hamp, B., Feldweg, H.: Germanet-a lexical-semantic net for german. In: Automatic Information Extraction and Building Of Lexical Semantic Resources for NLP Applications (1997)
Google Scholar
Hearst, M.A.: Automated discovery of wordnet relations. WordNet: an electronic lexical database, vol. 2 (1998)
Google Scholar
Hwang, C.H.: Incompletely and imprecisely speaking: using dynamic ontologies for representing and retrieving information. In: KRDB, vol. 21, pp. 14–20. Citeseer (1999)
Google Scholar
Jiang, Z., Xu, F.F., Araki, J., Neubig, G.: How can we know what language models know? Trans. Asso. Comput. Ling.
8
, 423–438 (2020).
https://doi.org/10.1162/tacl_a_00324
,
https://aclanthology.org/2020.tacl-1.28
Khan, L., Luo, F.: Ontology construction for information selection. In: 14th IEEE International Conference on Tools with Artificial Intelligence, 2002. (ICTAI 2002). Proceedings, pp. 122–127. IEEE (2002)
Google Scholar
Khot, T., et al.: Decomposed prompting: A modular approach for solving complex tasks (2023)
Google Scholar
Kietz, J.U., Maedche, A., Volz, R.: A method for semi-automatic ontology acquisition from a corporate intranet. In: EKAW-2000 Workshop “Ontologies and Text", Juan-Les-Pins, France (October 2000)
Google Scholar
Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models are zero-shot reasoners (2023)
Google Scholar
Konys, A.: Knowledge repository of ontology learning tools from text. Proc. Comput. Sci.
159
, 1614–1628 (2019)
Article
Google Scholar
Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-efficient prompt tuning. arXiv preprint
arXiv:2104.08691
(2021)
Levy, O., Seo, M., Choi, E., Zettlemoyer, L.: Zero-shot relation extraction via reading comprehension. In: Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pp. 333–342. Association for Computational Linguistics, Vancouver, Canada (Aug 2017).
https://doi.org/10.18653/v1/K17-1034
,
https://aclanthology.org/K17-1034
Lewis, M., et al.: Bart: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension (2019)
Google Scholar
Li, X.L., Liang, P.: Prefix-tuning: optimizing continuous prompts for generation. arXiv preprint
arXiv:2101.00190
(2021)
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing. ACM Comput. Surv.
55
(9) (2023).
https://doi.org/10.1145/3560815
Longpre, S., et al.: The flan collection: designing data and methods for effective instruction tuning. arXiv preprint
arXiv:2301.13688
(2023)
Lonsdale, D., Ding, Y., Embley, D.W., Melby, A.: Peppering knowledge sources with salt: Boosting conceptual content for ontology generation. In: Proceedings of the AAAI Workshop on Semantic Web Meets Language Resources, Edmonton, Alberta, Canada (2002)
Google Scholar
Lourdusamy, R., Abraham, S.: A survey on methods of ontology learning from text. In: Jain, L.C., Peng, S.-L., Alhadidi, B., Pal, S. (eds.) ICICCT 2019. LAIS, vol. 9, pp. 113–123. Springer, Cham (2020).
https://doi.org/10.1007/978-3-030-38501-9_11
Chapter
Google Scholar
Maedche, A., Staab, S.: Ontology learning for the semantic web. IEEE Intell. Syst.
16
(2), 72–79 (2001)
Article
Google Scholar
Medicomp Systems: MEDCIN (January 2023).
https://medicomp.com
Miller, G.A.: Wordnet: a lexical database for English. Commun. ACM
38
(11), 39–41 (1995)
Article
Google Scholar
Missikoff, M., Navigli, R., Velardi, P.: The usable ontology: an environment for building and assessing a domain ontology. In: Horrocks, I., Hendler, J. (eds.) ISWC 2002. LNCS, vol. 2342, pp. 39–53. Springer, Heidelberg (2002).
https://doi.org/10.1007/3-540-48005-6_6
Chapter
MATH
Google Scholar
Moldovan, D.I., GiRJU, R.C.: An interactive tool for the rapid development of knowledge bases. Inter. J. Artifi. Intell. Tools
10
(01n02), 65–86 (2001)
Google Scholar
National Cancer Institute, National Institutes of Health: NCI Thesaurus (September 2022).
http://ncit.nci.nih.gov
Noy, N.F., McGuinness, D.L., et al.: Ontology development 101: A guide to creating your first ontology (2001)
Google Scholar
OpenAI: Chatgpt (2023).
https://openai.com/chat-gpt/
(Accessed 5 May 2023)
OpenAI: Gpt-4 technical report (2023)
Google Scholar
Patel-Schneider, P.F.: Analyzing Schema.org. In: Mika, P., et al. (eds.) ISWC 2014. LNCS, vol. 8796, pp. 261–276. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-11964-9_17
Chapter
Google Scholar

## 한국어 번역

LLMs4OL: 온톨로지 학습을 위한 대규모 언어 모델 | 스프링거 네이처 링크
주요 콘텐츠로 건너뛰기
광고
로그인
메뉴
저널 찾기
우리와 함께 출판하세요
연구 추적
검색
저장된 연구
장바구니
홈
시맨틱 웹 – ISWC 2023
컨퍼런스 페이퍼
LLMs4OL: 온톨로지 학습을 위한 대규모 언어 모델
컨퍼런스 페이퍼
첫 번째 온라인:
2023년 10월 27일
408~427쪽
이 컨퍼런스 논문을 인용하세요
시맨틱 웹 – ISWC 2023
(ISWC 2023)
하메드 바바이 지글루
ORCID:
orcid.org/0000-0003-3758-1454
16
,
제니퍼 드수자
ORCID:
orcid.org/0000-0002-6616-9509
16
&
쇠렌 아우어
ORCID:
orcid.org/0000-0002-0698-2864
16
책 시리즈의 일부:
컴퓨터 과학 강의 노트
((LNCS, 14265권))
다음 컨퍼런스 시리즈에 포함되어 있습니다:
국제 시맨틱 웹 컨퍼런스
6198
액세스
111
인용
초록
우리는 온톨로지 학습(OL)을 위해 LLM(대형 언어 모델)을 활용하는 LLMs4OL 접근 방식을 제안합니다. LLM은 자연어 처리 분야에서 상당한 발전을 보여 다양한 지식 영역에서 복잡한 언어 패턴을 포착하는 능력을 입증했습니다. 우리의 LLMs4OL 패러다임은 다음 가설을 조사합니다.
LLM이 자연어 텍스트에서 지식을 자동으로 추출하고 구조화하는 언어 패턴 캡처 기능을 OL에 효과적으로 적용할 수 있습니까?
이 가설을 검증하기 위해 제로샷 프롬프트 방법을 사용하여 종합적인 평가를 수행합니다. 우리는 용어 입력, 분류학적 발견, 비분류학적 관계 추출이라는 세 가지 주요 OL 작업에 대해 9개의 서로 다른 LLM 모델군을 평가합니다. 또한 평가에는 WordNet의 어휘의미적 지식, GeoNames의 지리적 지식, UMLS의 의학 지식을 포함하여 다양한 장르의 존재론적 지식이 포함됩니다.
얻은 경험적 결과는 기초 LLM이 높은 수준의 추론 기술과 도메인 전문 지식을 수반하는 온톨로지 구성에 충분히 적합하지 않음을 보여줍니다. 그럼에도 불구하고, 효과적으로 미세 조정되면 온톨로지 구축을 위한 지식 획득 병목 현상을 완화하는 적절한 보조자 역할을 할 수 있습니다.
구독 콘텐츠 미리보기 입니다,
기관을 통해 로그인
액세스를 확인합니다.
이 장에 액세스
기관을 통해 로그인
구독하고 저장하세요
스프링거+
₩37.37/월부터
매월 10개의 장 또는 기사부터 시작
30만 권 이상의 도서와 2,500개 이상의 저널에서 장과 기사에 액세스하고 다운로드하세요.
언제든지 취소
계획 보기
지금 구매
장
EUR 29.95
가격에는 VAT가 포함되어 있습니다(대한민국)
PDF로 사용 가능
모든 기기에서 읽기
즉시 다운로드
영원히 소유하세요
챕터 구매
전자책
EUR 71.68
가격에는 VAT가 포함되어 있습니다(대한민국)
EPUB 및 PDF로 사용 가능
모든 기기에서 읽기
즉시 다운로드
영원히 소유하세요
eBook 구매
소프트커버 책
EUR 84.99
VAT 별도 가격 (대한민국)
컴팩트하고 가벼운 버전
영업일 기준 3~5일 이내에 발송됩니다.
전 세계 무료 배송 -
정보 보기
소프트커버 도서 구매
세금 계산은 결제 시 완료됩니다.
구매는 개인 용도로만 가능합니다.
기관 구독
다른 사람들이 유사한 콘텐츠를 보고 있음
LLMs4OM: 대규모 언어 모델과 온톨로지 일치
장
© 2025
LLM은 정말 도메인에 적응하나요? 온톨로지 학습 관점
장
© 2025
대규모 언어 모델을 위한 포괄적인 온톨로지 지식 평가 시스템
장
© 2025
관련 주제 탐색
머신러닝을 활용하여 추천되는 관련 주제의 최신 기사, 도서, 뉴스를 찾아보세요.
언어분석철학
전산언어학
온톨로지
선형 논리
자연어 처리(NLP)
언어 및 언어학 연구 방법
지식 그래프와 의미론적 데이터 통합
참고자료
Geonames 지리 데이터베이스(2023).
http://www.genames.org/
Agirre, E., Ansa, O., Hovy, E., Martínez, D.: www. In: 온톨로지 학습에 관한 제1차 국제 컨퍼런스 진행, vol. 31. pp. 25–30 (2000)
구글 학술검색
Akkalyoncu Yilmaz, Z., Wang, S., Yang, W., Zhang, H., Lin, J.: 자작나무를 사용한 문서 검색에 BERT 적용. In: 2019년 자연어 처리의 경험적 방법에 관한 회의 및 제9차 자연어 처리에 관한 국제 합동 회의(EMNLP-IJCNLP): 시스템 시연, 페이지 19-24. 전산언어학협회, 홍콩, 중국(2019년 11월).
https://doi.org/10.18653/v1/D19-3004
,
https://aclanthology.org/D19-3004
Alfonseca, E., Manandhar, S.: 일반적인 개체명 인식 및 자동화된 개념 발견을 위한 비지도 방법. In: 인도 마이소르에서 열린 제1차 일반 WordNet 국제 회의 간행물, pp. 34–43(2002)
구글 학술검색
Amatriain, X.: 변압기 모델: 소개 및 카탈로그. arXiv 사전 인쇄
arXiv:2302.07730
(2023)
Asim, M.N., Wasim, M., Khan, M.U.G., Mahmood, W., Abbasi, H.M.: 온톨로지 학습 기술 및 응용에 대한 조사. 데이터베이스 2018, bay101 (2018)
구글 학술검색
Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., Ives, Z.: DBpedia: 개방형 데이터 웹의 핵심입니다. In: Aberer, K., et al. (eds.) ASWC/ISWC -2007. LNCS, vol. 4825, 722~735페이지. 스프링거, 하이델베르그(2007).
https://doi.org/10.1007/978-3-540-76298-0_52
장
구글 학술검색
Bodenreider, O.: 통합 의료 언어 시스템(UMLS): 생물 의학 용어 통합. 핵산 해상도.
32
(suppl_1), D267–D270 2004).
https://doi.org/10.1093/nar/gkh061
Bodenreider, O.: 통합 의학 언어 시스템(umls): 생물 의학 용어 통합. 핵산 해상도.
32
(suppl_1), D267–D270 (2004)
구글 학술검색
Brown, T.B. 등: 언어 모델은 소수의 학습자입니다(2020)
구글 학술검색
Chung, H.W., et al.: 지침 미세 조정 언어 모델 확장(2022)
구글 학술검색
Cui, L., Wu, Y., Liu, J., Yang, S., Zhang, Y.: bart를 사용한 템플릿 기반 명명된 엔터티 인식. arXiv 사전 인쇄
arXiv:2106.01760
(2021)
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Salakhutdinov, R.: Transformer-xl: 고정 길이 컨텍스트를 넘어서는 세심한 언어 모델(2019)
구글 학술검색
Dalvi, F., Khan, A.R., Alam, F., Durrani, N., Xu, J., Sajjad, H.: BERT에서 학습된 잠재 개념 발견. In: 학습 표현에 관한 국제 컨퍼런스(2022).
https://openreview.net/forum?id=POTMtpYI1xH
Dettmers, T., Pasquale, M., Pontus, S., Riedel, S.: 컨볼루셔널 2D 지식 그래프 임베딩. In: 인공 지능에 관한 제32차 AAAI 회의 간행물, pp. 1811–1818(2018년 2월),
https://arxiv.org/abs/1707.01476
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: 언어 이해를 위한 심층 양방향 변환기 사전 훈련(2019)
구글 학술검색
Dopazo, J., Carazo, J.M.: 계통발생수의 토폴로지를 채택하는 감독되지 않은 성장 신경망을 사용한 계통발생적 재구성. J.Mol. 진화.
44
(2), 226–233 (1997)
기사
구글 학술검색
Gruber, T.R.: 지식 공유에 사용되는 온톨로지 설계 원칙을 향하여? 국제 J. Hum 컴퓨팅 스터드.
43
(5–6), 907–928 (1995)
기사
구글 학술검색
Gu, Y., et al.: 생물의학적 자연어 처리를 위한 도메인별 언어 모델 사전 훈련. ACM 트랜스. 계산. 헬스케어(건강)
3
(1), 1~23(2021)
구글 학술검색
Guha, R.V., Brickley, D., Macbeth, S.: 스키마. org: 웹에서 구조화된 데이터의 진화. 커뮤니케이터 ACM
59
(2), 44–51 (2016)
구글 학술검색
Hahn, U., Markó, K.G.: 문법 및 온톨로지를 위한 공동 지식 캡처. In: 제1차 지식 포착에 관한 국제 회의 진행, pp. 68–75(2001)
구글 학술검색
Hamp, B., Feldweg, H.: Germanet-독일어를 위한 어휘-의미 체계. In: NLP 애플리케이션을 위한 자동 정보 추출 및 어휘 의미 자원 구축(1997)
구글 학술검색
Hearst, M.A.: 워드넷 관계 자동 검색. WordNet: 전자 어휘 데이터베이스, vol. 2 (1998)
구글 학술검색
Hwang, C.H.: 불완전하고 부정확하게 말하면: 정보를 표현하고 검색하기 위해 동적 온톨로지를 사용합니다. In: KRDB, vol. 21, 14~20페이지. 시테시어 (1999)
구글 학술검색
Jiang, Z., Xu, F.F., Araki, J., Neubig, G.: 어떤 언어 모델이 알고 있는지 어떻게 알 수 있나요? 트랜스. 아소. 계산. 링.
8
, 423–438(2020).
https://doi.org/10.1162/tacl_a_00324
,
https://aclanthology.org/2020.tacl-1.28
Khan, L., Luo, F.: 정보 선택을 위한 온톨로지 구성. In: 2002년 인공 지능 도구에 관한 제14회 IEEE 국제 컨퍼런스(ICTAI 2002). 절차, pp. 122–127. IEEE (2002)
구글 학술검색
Khot, T. 등: 분해된 프롬프트: 복잡한 작업 해결을 위한 모듈식 접근 방식(2023)
구글 학술검색
Kietz, J.U., Maedche, A., Volz, R.: 기업 인트라넷에서 반자동 온톨로지 획득 방법. In: EKAW-2000 워크숍 "온톨로지 및 텍스트", Juan-Les-Pins, 프랑스(2000년 10월)
구글 학술검색
Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: 대형 언어 모델은 제로샷 추론기입니다(2023)
구글 학술검색
Konys, A.: 텍스트의 온톨로지 학습 도구에 대한 지식 저장소. 진행 계산. 과학.
159
, 1614~1628(2019)
기사
구글 학술검색
Lester, B., Al-Rfou, R., Constant, N.: 매개변수 효율적인 프롬프트 조정을 위한 규모의 힘. arXiv 사전 인쇄
arXiv:2104.08691
(2021)
Levy, O., Seo, M., Choi, E., Zettlemoyer, L.: 독해를 통한 제로샷 관계 추출. In: 제21회 컴퓨터 자연어 학습에 관한 컨퍼런스 진행(CoNLL 2017), pp. 333–342. 전산언어학협회, 캐나다 밴쿠버(2017년 8월).
https://doi.org/10.18653/v1/K17-1034
,
https://aclanthology.org/K17-1034
Lewis, M. 등: Bart: 자연어 생성, 번역 및 이해를 위한 시퀀스 간 사전 훈련(2019)
구글 학술검색
Li, X.L., Liang, P.: 접두사 조정: 생성을 위한 연속 프롬프트 최적화. arXiv 사전 인쇄
arXiv:2101.00190
(2021)
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: 사전 훈련, 프롬프트 및 예측: 자연어 처리에서 프롬프트 방법에 대한 체계적인 조사. ACM 컴퓨팅. 생존
55
(9) (2023).
https://doi.org/10.1145/3560815
Longpre, S., 외: 플랜 컬렉션: 효과적인 명령 조정을 위한 데이터 및 방법 설계. arXiv 사전 인쇄
arXiv:2301.13688
(2023)
Lonsdale, D., Ding, Y., Embley, D.W., Melby, A.: 소금으로 지식 소스 추가: 온톨로지 생성을 위한 개념 콘텐츠 강화. In: 시맨틱 웹과 언어 자원의 AAAI 워크숍 간행물, 캐나다 앨버타주 에드먼턴(2002)
구글 학술검색
Lourdusamy, R., Abraham, S.: 텍스트로부터 온톨로지 학습 방법에 대한 조사. In: Jain, L.C., Peng, S.-L., Alhadidi, B., Pal, S. (eds.) ICICCT 2019. LAIS, vol. 9, 113~123페이지. 스프링거, 참(2020).
https://doi.org/10.1007/978-3-030-38501-9_11
장
구글 학술검색
Maedche, A., Staab, S.: 시맨틱 웹을 위한 온톨로지 학습. IEEE 인텔. 시스템.
16
(2), 72–79 (2001)
기사
구글 학술검색
Medicomp 시스템: MEDCIN(2023년 1월).
https://medicomp.com
Miller, G.A.: Wordnet: 영어 어휘 데이터베이스입니다. 커뮤니케이터 ACM
38
(11), 39–41 (1995)
기사
구글 학술검색
Missikoff, M., Navigli, R., Velardi, P.: 사용 가능한 온톨로지: 도메인 온톨로지를 구축하고 평가하기 위한 환경입니다. In: Horrocks, I., Hendler, J. (eds.) ISWC 2002. LNCS, vol. 2342, pp. 39–53. 스프링거, 하이델베르그(2002).
https://doi.org/10.1007/3-540-48005-6_6
장
수학
구글 학술검색
Moldovan, D.I., GiRJU, R.C.: 지식 기반의 신속한 개발을 위한 대화형 도구입니다. 인터. J. Artifi. 인텔. 도구
10
(01n02), 65–86 (2001)
구글 학술검색
국립 암 연구소, 국립 보건원: NCI 유의어 사전(2022년 9월).
http://ncit.nci.nih.gov
Noy, N.F., McGuinness, D.L. 등: 온톨로지 개발 101: 첫 번째 온톨로지 생성 가이드(2001)
구글 학술검색
OpenAI: Chatgpt(2023).
https://openai.com/chat-gpt/
(2023년 5월 5일 액세스)
OpenAI: Gpt-4 기술 보고서(2023)
구글 학술검색
Patel-Schneider, P.F.: Schema.org 분석. In: Mika, P., et al. (eds.) ISWC 2014. LNCS, vol. 8796, pp. 261–276. 스프링거, 참(2014).
https://doi.org/10.1007/978-3-319-11964-9_17
장
구글 학술검색
